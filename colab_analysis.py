# â€”â€” å®Œæ•´æ•´åˆç‰ˆï¼šå­¸æ¥­å£“åŠ›èˆ‡æ†‚é¬±é¢¨éšªç›¸é—œæ€§åˆ†æ â€”â€”
# 1. ç³»çµ±èˆ‡å­—å‹è¨­å®š
from scipy.stats import chi2_contingency
from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy import stats
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from matplotlib.font_manager import FontProperties
import seaborn as sns
import numpy as np
import pandas as pd
import os
import platform
import warnings

warnings.filterwarnings(
    "ignore",
    message="Glyph .* missing from font\\(s\\) DejaVu Sans\\.",
    category=UserWarning,
    module="seaborn.utils"
)


# ğŸ“¦ è‹¥åœ¨ Colab ä¸Šï¼Œè‡ªå‹•å®‰è£ä¸­æ–‡å­—å‹
try:
    if "google.colab" in str(get_ipython()):
        print("ğŸ”§ Colab åµæ¸¬ä¸­ï¼Œå˜—è©¦å®‰è£ä¸­æ–‡å­—å‹...")
        os.system("apt-get install -y fonts-noto-cjk > /dev/null")
except:
    pass

# âœ… è·¨å¹³å°è‡ªå‹•é¸æ“‡ä¸­æ–‡å­—å‹


def load_chinese_font():
    system = platform.system()
    paths = []

    if system == "Windows":
        paths = [
            "C:/Windows/Fonts/msjh.ttc",
            "C:/Windows/Fonts/mingliu.ttc"
        ]
    elif system == "Darwin":  # macOS
        paths = [
            "/System/Library/Fonts/PingFang.ttc",
            "/System/Library/Fonts/STHeiti Medium.ttc"
        ]
    else:  # Linux / Colab
        paths = [
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"
        ]

    for path in paths:
        if os.path.exists(path):
            fm.fontManager.addfont(path)
            font = FontProperties(fname=path)
            plt.rcParams["font.family"] = font.get_name()
            print(f"âœ… å·²è¼‰å…¥ä¸­æ–‡å­—å‹ï¼š{font.get_name()}")
            return font

    print("âš ï¸ æ‰¾ä¸åˆ°ä¸­æ–‡å­—å‹ï¼Œä½¿ç”¨é è¨­è‹±æ–‡å­—å‹")
    return FontProperties()


# âœ… è¨­å®šå…¨åŸŸ zh_font
zh_font = load_chinese_font()
plt.rcParams["axes.unicode_minus"] = False
sns.set_style("whitegrid")


# 2. è³‡æ–™è®€å–èˆ‡åŸºæœ¬è™•ç†
# 2.1 ä¿ç•™åŸå§‹è³‡æ–™å‰¯æœ¬
raw_df = pd.read_csv('/content/student_depression_dataset.csv')
print(f"åŸå§‹è³‡æ–™é›†å¤§å°: {raw_df.shape}")
print(f"è³‡æ–™é›†åˆ—å: {raw_df.columns.tolist()}")

if raw_df['Depression'].dtype == object:
    raw_df['Depression'] = raw_df['Depression'].map(
        {'No': 0, 'Yes': 1}).astype(int)

# é¡¯ç¤ºåŸºæœ¬è³‡æ–™çµ±è¨ˆ
print("\nåŸºæœ¬è³‡æ–™çµ±è¨ˆ (æ•¸å€¼å‹è®Šæ•¸):")
print(raw_df[['Age', 'Academic Pressure', 'Work Pressure',
      'CGPA', 'Study Satisfaction']].describe())

# 2.2 è™•ç†å­¸æ¥­å£“åŠ›è®Šæ•¸ - ç¢ºä¿æ­£ç¢ºè®€å–
if 'Academic Pressure' not in raw_df.columns:
    # æª¢æŸ¥æ˜¯å¦æœ‰é¡ä¼¼åç¨±çš„æ¬„ä½
    possible_columns = [col for col in raw_df.columns if 'pressure' in col.lower(
    ) or 'academic' in col.lower()]
    if possible_columns:
        print(f"æ‰¾åˆ°å¯èƒ½çš„å­¸æ¥­å£“åŠ›ç›¸é—œæ¬„ä½: {possible_columns}")
        # å‡è¨­ç¬¬ä¸€å€‹æ˜¯å­¸æ¥­å£“åŠ›æ¬„ä½
        raw_df['Academic Pressure'] = raw_df[possible_columns[0]]
    else:
        print("æœªæ‰¾åˆ°å­¸æ¥­å£“åŠ›ç›¸é—œæ¬„ä½ï¼Œè«‹æª¢æŸ¥è³‡æ–™é›†")

# 2.3 Degreeè™•ç†èˆ‡å››ç´šåˆ†é¡
deg_mode = raw_df['Degree'].mode().iloc[0]
raw_df['Degree'] = raw_df['Degree'].fillna(deg_mode).astype(str).str.strip()
raw_df.loc[raw_df['Degree'] == 'å…¶ä»–', 'Degree'] = deg_mode


def simplify_degree(x):
    x = x.lower()
    if 'phd' in x or 'åšå£«' in x:
        return 'åšå£«'
    if 'master' in x or 'msc' in x or 'ç¢©å£«' in x:
        return 'ç¢©å£«'
    if any(k in x for k in ['bachelor', 'ba', 'b.sc', 'bsc', 'bcom', 'be', 'mba']):
        return 'å¤§å­¸'
    return 'é«˜ä¸­åŠä»¥ä¸‹'


raw_df['Degree4'] = raw_df['Degree'].apply(simplify_degree)
order4 = ['é«˜ä¸­åŠä»¥ä¸‹', 'å¤§å­¸', 'ç¢©å£«', 'åšå£«']

# 3. æ­£å¼è³‡æ–™è™•ç†
df = raw_df.copy().reset_index(drop=True)

# 3.1 å­¸æ¥­å£“åŠ›åˆ†é¡ - æ–°å¢
# ç¢ºä¿å­¸æ¥­å£“åŠ›æ˜¯æ•¸å€¼
if 'Academic Pressure' in df.columns:
    if df['Academic Pressure'].dtype == object:
        # å¦‚æœæ˜¯é¡åˆ¥è³‡æ–™ï¼Œè½‰æ›ç‚ºæ•¸å€¼
        ap_values = {k: i+1 for i,
                     k in enumerate(sorted(df['Academic Pressure'].unique()))}
        df['Academic Pressure_Value'] = df['Academic Pressure'].map(ap_values)
        # ä¿ç•™åŸå§‹é¡åˆ¥ä»¥ä¾¿åˆ†æ
        df['Academic Pressure_Category'] = df['Academic Pressure']
    else:
        # å¦‚æœå·²ç¶“æ˜¯æ•¸å€¼ï¼Œé€²è¡Œåˆ†çµ„
        df['Academic Pressure_Value'] = df['Academic Pressure']
        # ä½¿ç”¨ qcut å¯èƒ½å°è‡´ç©ºå€¼æˆ–ä¸å‡å‹»åˆ†å¸ƒçš„å•é¡Œï¼Œæ”¹ç”¨ cut
        # æª¢æŸ¥æ•¸æ“šçš„åˆ†ä½ˆ
        ap_min = df['Academic Pressure_Value'].min()
        ap_max = df['Academic Pressure_Value'].max()

        # ä½¿ç”¨ cut é€²è¡Œåˆ†çµ„ï¼Œç¢ºä¿æ¯å€‹çµ„æœ‰æ•¸æ“š
        bins = [ap_min, ap_min + (ap_max-ap_min)/3,
                ap_min + 2*(ap_max-ap_min)/3, ap_max]
        df['Academic Pressure_Category'] = pd.cut(
            df['Academic Pressure_Value'],
            bins=bins,
            labels=['ä½å£“åŠ›', 'ä¸­å£“åŠ›', 'é«˜å£“åŠ›'],
            include_lowest=True
        )
else:
    print("è­¦å‘Š: è³‡æ–™é›†ä¸­æ‰¾ä¸åˆ° Academic Pressure æ¬„ä½!")
    # å‰µå»ºä¸€å€‹æ›¿ä»£æ¬„ä½
    df['Academic Pressure_Value'] = df['Work Pressure']  # å‡è¨­å·¥ä½œå£“åŠ›å¯ä½œç‚ºæ›¿ä»£
    df['Academic Pressure_Category'] = pd.cut(
        df['Academic Pressure_Value'],
        bins=3,
        labels=['ä½å£“åŠ›', 'ä¸­å£“åŠ›', 'é«˜å£“åŠ›']
    )

# 3.2 æ›´åš´è¬¹çš„æ•¸æ“šæ¸…ç†
df = df.drop_duplicates().dropna(
    subset=['Academic Pressure_Value', 'Depression']).reset_index(drop=True)

# 3.3 æ•¸å€¼ç‰¹å¾µè™•ç†
num_cols = ['Age', 'Academic Pressure_Value',
            'Work Pressure', 'CGPA', 'Study Satisfaction']
df[num_cols] = df[num_cols].fillna(df[num_cols].median())
z = stats.zscore(df[num_cols])
df = df[(np.abs(z) < 3).all(axis=1)].reset_index(drop=True)
# scaler = StandardScaler()
# df_scaled = pd.DataFrame(
#     scaler.fit_transform(df[num_cols]),
#     columns=num_cols,
#     index=df.index
# )

# 3.4 å­¸æ­·åºæ•¸èˆ‡å¢å¼·ç‰¹å¾µ
df['degree_ord4'] = df['Degree4'].map({d: i+1 for i, d in enumerate(order4)})
df = pd.get_dummies(df, columns=['Gender'], drop_first=True)

# 4. å­¸æ¥­å£“åŠ›èˆ‡æ†‚é¬±é¢¨éšªç›¸é—œæ€§åˆ†æ

# 4.1 å­¸æ¥­å£“åŠ›èˆ‡æ†‚é¬±ç—‡çš„é—œè¯åˆ†æ - æ–°å¢
print("\n====== å­¸æ¥­å£“åŠ›èˆ‡æ†‚é¬±é¢¨éšªç›¸é—œæ€§åˆ†æ ======")
ap_corr = df['Academic Pressure_Value'].corr(df['Depression'])
print(f"å­¸æ¥­å£“åŠ›èˆ‡æ†‚é¬±ç—‡çš„ç›¸é—œä¿‚æ•¸: {ap_corr:.3f}")

# 4.2 ä¸åŒå£“åŠ›æ°´å¹³çš„æ†‚é¬±é¢¨éšªåˆ†æ - æ–°å¢
ap_group = df.groupby('Academic Pressure_Category', observed=False)[
    'Depression'].agg(['mean', 'count'])

ap_group.columns = ['æ†‚é¬±æ¯”ä¾‹', 'æ¨£æœ¬æ•¸']
print("\nä¸åŒå­¸æ¥­å£“åŠ›æ°´å¹³çš„æ†‚é¬±é¢¨éšª:")
print(ap_group)

# è¨ºæ–·å’Œä¿®å¾©åœ–è¡¨é¡¯ç¤ºå•é¡Œ
print("\n====== è¨ºæ–·åœ–è¡¨é¡¯ç¤ºå•é¡Œ ======")

# æª¢æŸ¥åŸå§‹è³‡æ–™
print(f"åŸå§‹è³‡æ–™å¤§å°: {df.shape}")
print(
    f"Academic Pressure_Value ç¯„åœ: {df['Academic Pressure_Value'].min()} åˆ° {df['Academic Pressure_Value'].max()}")
print(f"æ†‚é¬±è®Šæ•¸ (Depression) å€¼è¨ˆæ•¸: \n{df['Depression'].value_counts()}")

# æª¢æŸ¥åˆ†çµ„è³‡æ–™
print("\nå­¸æ¥­å£“åŠ›åˆ†çµ„æƒ…æ³:")
ap_category_counts = df['Academic Pressure_Category'].value_counts()
print(ap_category_counts)

# æª¢æŸ¥ä½ã€ä¸­ã€é«˜å£“åŠ›çµ„çš„æ†‚é¬±æ¯”ä¾‹
print("\nå„å£“åŠ›çµ„çš„æ†‚é¬±æ¯”ä¾‹:")
for category in df['Academic Pressure_Category'].unique():
    subset = df[df['Academic Pressure_Category'] == category]
    depression_rate = subset['Depression'].mean()
    count = len(subset)
    print(f"{category}: æ†‚é¬±æ¯”ä¾‹ = {depression_rate:.4f}, æ¨£æœ¬æ•¸ = {count}")

# ä½¿ç”¨æœ€åŸºæœ¬çš„æ–¹å¼ç¢ºä¿è¦–è¦ºåŒ–èƒ½å¤ å·¥ä½œ
plt.figure(figsize=(16, 6))

# === ç¬¬ä¸€å€‹åœ–ï¼šæœ€ç°¡å–®çš„æ¢å½¢åœ– ===
plt.subplot(1, 2, 1)

# ç›´æ¥å‰µå»ºä¸‰å€‹åˆ†é¡å’Œå°æ‡‰çš„æ†‚é¬±æ¯”ä¾‹
categories = ['ä½å£“åŠ›', 'ä¸­å£“åŠ›', 'é«˜å£“åŠ›']
depression_rates = []

# æ‰‹å‹•è¨ˆç®—å„é¡åˆ¥çš„æ†‚é¬±æ¯”ä¾‹
for category in categories:
    if category in df['Academic Pressure_Category'].values:
        subset = df[df['Academic Pressure_Category'] == category]
        rate = subset['Depression'].mean()
    else:
        rate = 0  # å¦‚æœé¡åˆ¥ä¸å­˜åœ¨ï¼Œè¨­ç‚º0
    depression_rates.append(rate)

# æ‰“å°æª¢æŸ¥è³‡æ–™
print("\nç”¨æ–¼ç¹ªåœ–çš„è³‡æ–™:")
for cat, rate in zip(categories, depression_rates):
    print(f"{cat}: {rate:.4f}")

# ä½¿ç”¨æœ€åŸºæœ¬çš„plt.barç¹ªè£½æ¢å½¢åœ–
bars = plt.bar(categories, depression_rates, color=['blue', 'green', 'red'])

# æ·»åŠ æ•¸æ“šæ¨™ç±¤
for bar in bars:
    height = bar.get_height()
    plt.text(
        bar.get_x() + bar.get_width()/2.,
        height + 0.02,
        f'{height:.2f}',
        ha='center',
        fontsize=12
    )

plt.xticks(fontproperties=zh_font, fontsize=14)
plt.yticks(fontproperties=zh_font, fontsize=14)
plt.xlabel('å­¸æ¥­å£“åŠ›æ°´å¹³', fontproperties=zh_font)
plt.ylabel('æ†‚é¬±æ¯”ä¾‹', fontproperties=zh_font)
plt.title('ä¸åŒå­¸æ¥­å£“åŠ›æ°´å¹³çš„æ†‚é¬±æ¯”ä¾‹', fontproperties=zh_font)
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# === ç¬¬äºŒå€‹åœ–ï¼šå­¸æ¥­å£“åŠ›é€£çºŒå€¼çš„åˆ†çµ„æŸ±ç‹€åœ– ===
plt.subplot(1, 2, 2)

# å‰µå»ºç°¡å–®çš„åˆ†çµ„ - åªåˆ†æˆ5çµ„ä»¥ç¢ºä¿æ¯çµ„æœ‰è¶³å¤ è³‡æ–™
n_bins = 5
min_val = df['Academic Pressure_Value'].min()
max_val = df['Academic Pressure_Value'].max()
bin_edges = np.linspace(min_val, max_val, n_bins + 1)

# å‰µå»ºåˆ†çµ„æ¨™ç±¤
bin_labels = [
    f'{bin_edges[i]:.1f}-{bin_edges[i+1]:.1f}' for i in range(n_bins)]

# æ‰‹å‹•åˆ†çµ„ä¸¦è¨ˆç®—æ¯çµ„çš„æ†‚é¬±æ¯”ä¾‹
depression_by_bin = []
counts_by_bin = []

for i in range(n_bins):
    if i < n_bins - 1:
        mask = (df['Academic Pressure_Value'] >= bin_edges[i]) & (
            df['Academic Pressure_Value'] < bin_edges[i+1])
    else:  # æœ€å¾Œä¸€çµ„åŒ…å«å³é‚Šç•Œ
        mask = (df['Academic Pressure_Value'] >= bin_edges[i]) & (
            df['Academic Pressure_Value'] <= bin_edges[i+1])

    group_data = df[mask]
    count = len(group_data)
    counts_by_bin.append(count)

    if count > 0:
        depression_rate = group_data['Depression'].mean()
    else:
        depression_rate = 0

    depression_by_bin.append(depression_rate)

# æ‰“å°ç”¨æ–¼ç¹ªåœ–çš„è³‡æ–™
print("\nå­¸æ¥­å£“åŠ›é€£çºŒå€¼åˆ†çµ„:")
for label, rate, count in zip(bin_labels, depression_by_bin, counts_by_bin):
    print(f"{label}: æ†‚é¬±æ¯”ä¾‹ = {rate:.4f}, æ¨£æœ¬æ•¸ = {count}")

# ç¢ºä¿æ‰€æœ‰æ•¸å€¼æœ‰æ•ˆï¼Œé¿å…ç©ºç™½åœ–è¡¨
if all(count > 0 for count in counts_by_bin) and any(rate > 0 for rate in depression_by_bin):
    # ä½¿ç”¨æœ€åŸºæœ¬çš„plt.barç¹ªè£½æ¢å½¢åœ–
    bars = plt.bar(bin_labels, depression_by_bin, color=[
                   'skyblue', 'lightgreen', 'lightsalmon', 'lightpink', 'lightgoldenrodyellow'])

    # æ·»åŠ æ•¸æ“šæ¨™ç±¤
    for bar, count in zip(bars, counts_by_bin):
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width()/2.,
            height + 0.02,
            f'{height:.2f}\nn={count}',
            ha='center',
            fontsize=10
        )

    plt.title('å­¸æ¥­å£“åŠ›æŒ‡æ•¸èˆ‡æ†‚é¬±é¢¨éšªé—œä¿‚', fontproperties=zh_font, fontsize=16)
    plt.xlabel('å­¸æ¥­å£“åŠ›æŒ‡æ•¸å€é–“', fontproperties=zh_font, fontsize=14)
    plt.ylabel('æ†‚é¬±æ¯”ä¾‹', fontproperties=zh_font, fontsize=14)
    plt.ylim(0, 1)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.xticks(rotation=30, fontproperties=zh_font, ha='right')
else:
    plt.text(0.5, 0.5, 'ç„¡è¶³å¤ è³‡æ–™ç”Ÿæˆåœ–è¡¨',
             horizontalalignment='center',
             verticalalignment='center',
             fontproperties=zh_font, fontsize=16)
    print("è­¦å‘Š: ç„¡æ³•ç”Ÿæˆåœ–è¡¨ï¼Œå› ç‚ºè³‡æ–™åˆ†çµ„å¾Œæ¨£æœ¬æ•¸ä¸è¶³æˆ–æ†‚é¬±æ¯”ä¾‹å…¨ç‚º0")

plt.tight_layout()

# â¶ å®šç¾© contingency_table
contingency_table = pd.crosstab(
    df['Academic Pressure_Category'],
    df['Depression']
)

print("\näº¤å‰åˆ—è¯è¡¨ï¼š")
print(contingency_table)

# ä½¿ç”¨å¡æ–¹æª¢å®š
chi2, p_value, dof, expected = chi2_contingency(contingency_table)
print("\nä¸åŒå£“åŠ›ç­‰ç´šçš„æ†‚é¬±é¢¨éšªå·®ç•°æª¢å®š (å¡æ–¹æª¢å®š):")
print(f"å¡æ–¹å€¼: {chi2:.3f}, è‡ªç”±åº¦: {dof}, p-value: {p_value:.4f}")
print(f"çµè«–: {'å£“åŠ›ç­‰ç´šä¹‹é–“æ†‚é¬±é¢¨éšªæœ‰é¡¯è‘—å·®ç•°' if p_value < 0.05 else 'å£“åŠ›ç­‰ç´šä¹‹é–“æ†‚é¬±é¢¨éšªæ²’æœ‰é¡¯è‘—å·®ç•°'}")

# 4.6 PCAåˆ†æï¼ˆè€ƒæ…®å­¸æ¥­å£“åŠ›ï¼‰
features_all = ['degree_ord4', 'Academic Pressure_Value'] + \
    [c for c in df.columns if c.startswith('Gender_')] + num_cols
pca = PCA(n_components=min(5, len(features_all)))
pca.fit(df[features_all])
load = pd.Series(pca.components_[0], index=features_all).abs(
).sort_values(ascending=False)
print("\nPCAåˆ†æï¼ˆä¸»æˆåˆ†åˆ†æï¼‰:")
print("PC1 è§£é‡‹è®Šç•°æ¯”ä¾‹ï¼š", np.round(pca.explained_variance_ratio_[0], 3))
print("PC1 è¼‰è·é‡æ’åºï¼ˆå‰5é …ï¼‰ï¼š\n", load.head(5).round(3))
print("â†’ å­¸æ¥­å£“åŠ›åœ¨PC1ä¸­çš„é‡è¦æ€§æ’åï¼š", list(load.index).index('Academic Pressure_Value') + 1)

# 5. å­¸æ¥­å£“åŠ›ç‰¹å®šåˆ†æ - æ–°å¢

# 5.1 å­¸æ¥­å£“åŠ›èˆ‡å…¶ä»–è®Šæ•¸çš„äº¤äº’ä½œç”¨
print("\n====== å­¸æ¥­å£“åŠ›èˆ‡å…¶ä»–è®Šæ•¸çš„äº¤äº’ä½œç”¨ ======")

# è¨ˆç®—å­¸æ¥­å£“åŠ›èˆ‡å…¶ä»–æ•¸å€¼è®Šæ•¸çš„ç›¸é—œæ€§
corr_with_ap = df[num_cols].corr(
)['Academic Pressure_Value'].sort_values(ascending=False)
print("å­¸æ¥­å£“åŠ›èˆ‡å…¶ä»–è®Šæ•¸çš„ç›¸é—œæ€§:")
print(corr_with_ap)

# 6. æ¨¡å‹å»ºç«‹èˆ‡è©•ä¼°
scaler = StandardScaler()  # â† âœ… åŠ ä¸Šé€™è¡Œå°±ä¸æœƒå ± NameError

# 6.1 æº–å‚™ç‰¹å¾µèˆ‡æ¨™ç±¤ï¼Œä¸¦æ¨™æº–åŒ–
features = ['Academic Pressure_Value', 'degree_ord4'] + [c for c in df.columns if c.startswith(
    'Gender_')] + [col for col in num_cols if col != 'Academic Pressure_Value']
X = scaler.fit_transform(df[features])
y = df['Depression']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 6.2 Logistic Regressionæ¨¡å‹
print("\n====== é æ¸¬æ¨¡å‹å»ºç«‹èˆ‡è©•ä¼° ======")
print("\nLogistic Regressionæ¨¡å‹:")
lr = LogisticRegression(
    class_weight='balanced',
    solver='liblinear',    # ç”¨ liblinear æ¼”ç®—æ³•æ›´ç©©å®š
    max_iter=5000,         # å¤šè·‘å¹¾æ¬¡è¿­ä»£
    random_state=42
)

lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
y_proba = lr.predict_proba(X_test)[:, 1]  # type: ignore[index]

# 6.3 æ··æ·†çŸ©é™£åˆ†æ
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=['é æ¸¬ 0ï¼ˆç„¡æ†‚é¬±ï¼‰', 'é æ¸¬ 1ï¼ˆæœ‰æ†‚é¬±ï¼‰'],  # type: ignore[arg-type]
    yticklabels=['å¯¦éš› 0ï¼ˆç„¡æ†‚é¬±ï¼‰', 'å¯¦éš› 1ï¼ˆæœ‰æ†‚é¬±ï¼‰'],  # type: ignore[arg-type]
    annot_kws={'fontproperties': zh_font, 'fontsize': 11}
)
plt.title('Logistic Regression æ··æ·†çŸ©é™£', fontproperties=zh_font, fontsize=14)
plt.ylabel('å¯¦éš›æ¨™ç±¤', fontproperties=zh_font, fontsize=12)
plt.xlabel('é æ¸¬æ¨™ç±¤', fontproperties=zh_font, fontsize=12)
plt.xticks(fontproperties=zh_font, rotation=0)
plt.yticks(fontproperties=zh_font, rotation=0)
plt.tight_layout()
plt.show()

# 6.4 æ¨¡å‹æ€§èƒ½è©³ç´°è©•ä¼°
acc = accuracy_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)
print(f"å¤šç‰¹å¾µ Logistic Regression æº–ç¢ºç‡ï¼š{acc:.3f}ï¼ŒAUCï¼š{auc:.3f}")
print("\nåˆ†é¡å ±å‘Šï¼š")
print(classification_report(
    y_test,
    y_pred,
    zero_division=0        # é æ¸¬ä¸åˆ°æŸé¡åˆ¥æ™‚å›å‚³ 0ï¼Œä¸å‡º warning
))

# 6.5 ç‰¹å¾µé‡è¦æ€§åˆ†æ
importance = pd.Series(
    np.abs(lr.coef_[0]), index=features).sort_values(ascending=False)
plt.figure(figsize=(10, 6))
importance.plot(kind='bar')
plt.title('Logistic Regression ç‰¹å¾µé‡è¦æ€§', fontproperties=zh_font, fontsize=14)
plt.xlabel('ç‰¹å¾µ', fontproperties=zh_font, fontsize=12)
plt.ylabel('ä¿‚æ•¸çµ•å°å€¼', fontproperties=zh_font, fontsize=12)
plt.xticks(rotation=45, ha='right', fontproperties=zh_font)
plt.tight_layout()
plt.show()

# 6.6 éš¨æ©Ÿæ£®æ—æ¨¡å‹èˆ‡ç‰¹å¾µé‡è¦æ€§æ¯”è¼ƒ - æ–°å¢
print("\néš¨æ©Ÿæ£®æ—æ¨¡å‹:")
rf = RandomForestClassifier(
    n_estimators=100, random_state=42, class_weight='balanced')
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
rf_proba = rf.predict_proba(X_test)[:, 1]

rf_acc = accuracy_score(y_test, rf_pred)
rf_auc = roc_auc_score(y_test, rf_proba)
print(f"éš¨æ©Ÿæ£®æ—æ¨¡å‹æº–ç¢ºç‡: {rf_acc:.3f}, AUC: {rf_auc:.3f}")

# è¨ˆç®—éš¨æ©Ÿæ£®æ—ç‰¹å¾µé‡è¦æ€§
result = permutation_importance(      # type: ignore[call-arg]
    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1
)
rf_importance = pd.Series(
    result.importances_mean,          # type: ignore[attr-defined]
    index=features
).sort_values(ascending=False)

# 6.7 å…©ç¨®æ¨¡å‹çš„å­¸æ¥­å£“åŠ›é‡è¦æ€§æ¯”è¼ƒ
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
importance.head(5).plot(kind='bar')
plt.title('LRæ¨¡å‹ç‰¹å¾µé‡è¦æ€§ (Top 5)', fontproperties=zh_font, fontsize=14)
plt.xlabel('ç‰¹å¾µ', fontproperties=zh_font, fontsize=12)
plt.ylabel('ä¿‚æ•¸çµ•å°å€¼', fontproperties=zh_font, fontsize=12)
plt.xticks(rotation=45, ha='right', fontproperties=zh_font)

plt.subplot(1, 2, 2)
rf_importance.head(5).plot(kind='bar')
plt.title('RFæ¨¡å‹ç‰¹å¾µé‡è¦æ€§ (Top 5)', fontproperties=zh_font, fontsize=14)
plt.xlabel('ç‰¹å¾µ', fontproperties=zh_font, fontsize=12)
plt.ylabel('é‡è¦æ€§åˆ†æ•¸', fontproperties=zh_font, fontsize=12)
plt.xticks(rotation=45, ha='right', fontproperties=zh_font)

plt.tight_layout()
plt.show()

# 6.8 ROCæ›²ç·šæ¯”è¼ƒ - æ–°å¢
plt.figure(figsize=(8, 6))
# Logistic Regression ROC
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.plot(fpr, tpr, label=f'LR (AUC = {auc:.3f})')

# Random Forest ROC
rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_proba)
plt.plot(rf_fpr, rf_tpr, label=f'RF (AUC = {rf_auc:.3f})')

# å°è§’ç·š
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('å½é™½æ€§ç‡ (FPR)', fontproperties=zh_font, fontsize=12)
plt.ylabel('çœŸé™½æ€§ç‡ (TPR)', fontproperties=zh_font, fontsize=12)
plt.title('é æ¸¬æ†‚é¬±é¢¨éšªçš„ROCæ›²ç·šæ¯”è¼ƒ', fontproperties=zh_font, fontsize=14)
plt.legend(loc="lower right", prop=zh_font)
plt.grid(True)
plt.show()

# 7. ç ”ç©¶çµè«–èˆ‡å»ºè­° - æ–°å¢
print("\n====== ç ”ç©¶çµè«–èˆ‡å»ºè­° ======")
# å­¸æ¥­å£“åŠ›çš„é‡è¦æ€§è©•ä¼°
ap_lr_rank = list(importance.index).index('Academic Pressure_Value') + \
    1 if 'Academic Pressure_Value' in importance.index else "æœªåœ¨ç‰¹å¾µä¸­"
ap_rf_rank = list(rf_importance.index).index('Academic Pressure_Value') + \
    1 if 'Academic Pressure_Value' in rf_importance.index else "æœªåœ¨ç‰¹å¾µä¸­"

print(f"\n1. å­¸æ¥­å£“åŠ›èˆ‡æ†‚é¬±é¢¨éšªçš„ç›¸é—œä¿‚æ•¸ç‚º {ap_corr:.3f}")
if ap_corr > 0:
    print("   å­¸æ¥­å£“åŠ›è¶Šé«˜ï¼Œæ†‚é¬±é¢¨éšªå‚¾å‘æ–¼å¢åŠ ")
elif ap_corr < 0:
    print("   å­¸æ¥­å£“åŠ›è¶Šé«˜ï¼Œæ†‚é¬±é¢¨éšªå‚¾å‘æ–¼æ¸›å°‘")
else:
    print("   å­¸æ¥­å£“åŠ›èˆ‡æ†‚é¬±é¢¨éšªæ²’æœ‰æ˜é¡¯çš„ç·šæ€§é—œä¿‚")

print(f"\n2. å­¸æ¥­å£“åŠ›åœ¨é æ¸¬æ†‚é¬±é¢¨éšªçš„ç‰¹å¾µä¸­:")
print(f"   - åœ¨Logistic Regressionæ¨¡å‹ä¸­æ’åç¬¬{ap_lr_rank}ä½")
print(f"   - åœ¨éš¨æ©Ÿæ£®æ—æ¨¡å‹ä¸­æ’åç¬¬{ap_rf_rank}ä½")

print("\n3. ä¸åŒå­¸æ¥­å£“åŠ›ç´šåˆ¥çš„æ†‚é¬±é¢¨éšª:")
for level, row in ap_group.iterrows():
    if isinstance(row, pd.Series):
        print(f"   - {level}: {row['æ†‚é¬±æ¯”ä¾‹']:.2%} (æ¨£æœ¬æ•¸: {row['æ¨£æœ¬æ•¸']})")

print("\n4. ç ”ç©¶å»ºè­°:")
print("   - æ ¹æ“šåˆ†æçµæœï¼Œæ‡‰è©²æä¾›æ›´å¤šçš„å¿ƒç†å¥åº·è³‡æºçµ¦é«˜å­¸æ¥­å£“åŠ›å­¸ç”Ÿ")
print("   - å­¸æ ¡å¯ä»¥ç™¼å±•å£“åŠ›ç®¡ç†åŸ¹è¨“èª²ç¨‹ï¼Œç‰¹åˆ¥é‡å°é«˜é¢¨éšªå­¸ç”Ÿç¾¤é«”")
print("   - é€²ä¸€æ­¥ç ”ç©¶å¯ä»¥æ¢ç´¢å­¸æ¥­å£“åŠ›èˆ‡å…¶ä»–å› ç´ ï¼ˆå¦‚ç¤¾äº¤æ”¯æŒã€ç¡çœ è³ªé‡ï¼‰çš„äº¤äº’ä½œç”¨")
